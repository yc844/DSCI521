{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring modules for Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Very simple testing for project :\n",
    "- textblob \n",
    "- spacy \n",
    "Neither were very good for sentiment analysis \n",
    "Could be used to add features like Named Entity Recognition (NER) and POS \n",
    "- word2vec setup \n",
    "- glove with pip install needed to change to installed directory, probably could be fixed with PYTHONPATH change \n",
    "- local glove example \n",
    "Not sure there's a real advantage to word2vec and glove but we could test... \n",
    "To-do - n-grams \n",
    "\n",
    "Warning: the directories are local to my machine because didn't use \\data directory and then installed a package that I \n",
    "coulnd't get to work in the current jupyter notebook... \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run from top of directory \n",
    "#will need to change for local install of glove \n",
    "#https://stackoverflow.com/questions/50044615/import-local-module-in-jupyter-notebook\n",
    "\n",
    "import os \n",
    "#if you want to know current working dir\n",
    "os.getcwd()\n",
    "#if you want to change\n",
    "os.chdir(r'C:\\\\DSCI521\\\\project')  #<=================== where the local zip package is installed \n",
    "# # if you want to list dir\n",
    "# os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # real data \n",
    "# df_train=pd.read_csv('train.csv')\n",
    "# df_traintest=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "#sample small set of real data \n",
    "s =pd.read_csv('sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.info() #30 entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('a').count()\n",
    "s.groupby('location').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= s.iloc[14]['text']  #to practice on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned up \n",
    "t = \"Accident in #Ashville on US 23 SB before SR 752 #traffic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#easy sentiment analysis \n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.words  #removed the #!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "blob = TextBlob(t, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# using Spacy for NER \n",
    "############################# \n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Accident in #Ashville on US 23 SB before SR 752 #traffic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nlp(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in d:\n",
    "    print('text:',token.text+'\\n', 'token_lemma:',token.lemma_, 'token_pos:',token.pos_, 'tag:',token.tag_, 'dep:',token.dep_,\n",
    "            'shape:',token.shape_, 'token is_alpha',token.is_alpha, 'stop tf:', token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### note that this is d.ents ... ents matter! \n",
    "for ent in d.ents:  #from spacy docs https://spacy.io/usage/linguistic-features\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "#note that it got Ashville North Carolina wrong as a PERSON instead of City... US is road designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# word2Vector \n",
    "#https://code.google.com/archive/p/word2vec/ \n",
    "#needs cython \n",
    "# pip install word2vec WILL NOT PIP INSTALL \n",
    "##################################################################\n",
    "!pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# word2vec from gensim \n",
    "# https://radimrehurek.com/gensim/models/word2vec.html for example\n",
    "# tutorial https://rare-technologies.com/word2vec-tutorial/ \n",
    "############################### will not install! ##############\n",
    "!pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['computer']  # numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# GloVe \n",
    "# http://www.foldl.me/2014/glove-python/#implementation   \n",
    "#https://github.com/JonathanRaiman/glove \n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/50044615/import-local-module-in-jupyter-notebook\n",
    "\n",
    "import os \n",
    "#if you want to know current working dir\n",
    "os.getcwd()\n",
    "#if you want to change\n",
    "os.chdir('C:\\\\mingw64\\\\glove-master')  #<=================== where the local zip package is installed \n",
    "# # if you want to list dir\n",
    "# os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data \n",
    "#https://github.com/JonathanRaiman/glove\n",
    "cooccur = {\n",
    "\t0: {\n",
    "\t\t0: 1.0,\n",
    "\t\t2: 3.5\n",
    "\t},\n",
    "\t1: {\n",
    "\t\t2: 0.5\n",
    "\t},\n",
    "\t2: {\n",
    "\t\t0: 3.5,\n",
    "\t\t1: 0.5,\n",
    "\t\t2: 1.2\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = glove.Glove(cooccur, d=50, alpha=0.75, x_max=100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    err = model.train(batch_size=200,workers=9)\n",
    "#    err = model.train(batch_size=200, workers=9, batch_size=50) #batch size repeated... \n",
    "    print(\"epoch %d, error %.3f\" % (epoch, err), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# installing GloVe \n",
    "#https://github.com/stanfordnlp/GloVe \n",
    "#directions for running make https://github.com/maciejkula/glove-python/wiki/Installation-on-Windows - don't do!\n",
    "#https://nlp.stanford.edu/projects/glove/ \n",
    "#\n",
    "# refresher on gcc and makefile \n",
    "# https://www3.ntu.edu.sg/home/ehchua/programming/cpp/gcc_make.html\n",
    "# http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/\n",
    "# https://www.gnu.org/software/make/manual/make.html \n",
    "#\n",
    "#setup mingw64, unzip glove, run make.sh \n",
    "#embeddings repo http://vectors.nlpl.eu/repository/# \n",
    "# http://www.foldl.me/2014/glove-python/#implementation  - \n",
    "\n",
    "#  did not work! so skip it? \n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# local version based on Bruno presentation on OReilly \n",
    "#Bruno GonÃ§alves\n",
    "#www.data4sci.com\n",
    "#@bgoncalves, @data4sci\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurence_matrix(word_dict, text_words, window_size=1):\n",
    "    vocabulary_size = len(word_dict)\n",
    "    matrix = np.zeros((vocabulary_size, vocabulary_size), dtype='int')\n",
    "\n",
    "    for i in range(window_size+1, len(text_words)-window_size):\n",
    "        word_id = word_dict[text_words[i]]\n",
    "        \n",
    "        for j in range(i-window_size, i+window_size+1):\n",
    "            if j == i: \n",
    "                continue\n",
    "            \n",
    "            context_id = word_dict[text_words[j]]\n",
    "            \n",
    "            matrix[word_id, context_id] += 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words = ['mary', 'had', 'a', 'little', 'lamb', 'little', 'lamb', 'little', 'lamb', 'mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'mary', 'went', 'mary', 'went', 'mary', 'went', 'everywhere', 'that', 'mary', 'went', 'the', 'lamb', 'was', 'sure', 'to', 'go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_word_dict = {'had': 1,\n",
    " 'sure': 16,\n",
    " 'snow': 10,\n",
    " 'that': 13,\n",
    " 'went': 14,\n",
    " 'fleece': 6,\n",
    " 'everywhere': 12,\n",
    " 'the': 15,\n",
    " 'to': 17,\n",
    " 'mary': 0,\n",
    " 'lamb': 4,\n",
    " 'was': 7,\n",
    " 'white': 8,\n",
    " 'and': 11,\n",
    " 'a': 2,\n",
    " 'little': 3,\n",
    " 'go': 18,\n",
    " 'whose': 5,\n",
    " 'as': 9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = cooccurence_matrix(mary_word_dict, text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "temp = matrix.astype('str')\n",
    "temp[temp=='0'] = \"\"\n",
    "pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob = matrix/matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob[mary_word_dict['mary'], mary_word_dict['had']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob[mary_word_dict['mary'], mary_word_dict['lamb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
